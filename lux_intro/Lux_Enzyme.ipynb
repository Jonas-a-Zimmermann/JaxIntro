{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we want to introduce an alternative to JAX&Flax: namely Enzyme&Reactant, used in conjunction with Lux as a ML library. For this, we reimplement the good old MLP MNIST problem once more.\n",
    "Note that the role of JAX is not only AD, but also to take code, and compile it down to XLA (@jax.jit). This is done by Reactant. The AD part itself is run by Enzyme.\n",
    "\n",
    "Note: Due to some CUDA.jl issues, we need atleast Julia version 1.11.2. I recommend 1.11.3, for other versions this notebook is not tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/jonas/Documents/project/jax_intro/lux_intro/lux_intro_env/Project.toml\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV[\"XLA_FLAGS\"] = \"--xla_gpu_enable_triton_gemm=false\"\n",
    "Base.active_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739379197.046268   64843 service.cc:152] XLA service 0x398c32a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739379197.046296   64843 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1739379197.046299   64843 service.cc:160]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1739379197.046301   64843 service.cc:160]   StreamExecutor device (2): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1739379197.046302   64843 service.cc:160]   StreamExecutor device (3): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1739379197.047911   64843 se_gpu_pjrt_client.cc:987] Using BFC allocator.\n",
      "I0000 00:00:1739379197.047941   64843 gpu_helpers.cc:136] XLA backend allocating 18946572288 bytes on device 0 for BFCAllocator.\n",
      "I0000 00:00:1739379197.047968   64843 gpu_helpers.cc:136] XLA backend allocating 18946572288 bytes on device 1 for BFCAllocator.\n",
      "I0000 00:00:1739379197.047982   64843 gpu_helpers.cc:136] XLA backend allocating 18946572288 bytes on device 2 for BFCAllocator.\n",
      "I0000 00:00:1739379197.047995   64843 gpu_helpers.cc:136] XLA backend allocating 18920128512 bytes on device 3 for BFCAllocator.\n",
      "I0000 00:00:1739379197.048008   64843 gpu_helpers.cc:177] XLA backend will use up to 6315524096 bytes on device 0 for CollectiveBFCAllocator.\n",
      "I0000 00:00:1739379197.048020   64843 gpu_helpers.cc:177] XLA backend will use up to 6315524096 bytes on device 1 for CollectiveBFCAllocator.\n",
      "I0000 00:00:1739379197.048032   64843 gpu_helpers.cc:177] XLA backend will use up to 6315524096 bytes on device 2 for CollectiveBFCAllocator.\n",
      "I0000 00:00:1739379197.048043   64843 gpu_helpers.cc:177] XLA backend will use up to 6306709504 bytes on device 3 for CollectiveBFCAllocator.\n",
      "I0000 00:00:1739379197.068254   64843 cuda_dnn.cc:529] Loaded cuDNN version 90400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reactant.XLA.Client(Ptr{Nothing} @0x0000000039fce750, Int32[0, 1, 2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Lux, LuxCUDA, Random, Optimisers, Plots, Reactant, Enzyme, OneHotArrays, Zygote\n",
    "Reactant.set_default_backend(\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xoshiro(0xfefa8d41b8f5dca5, 0xf80cc98e147960c1, 0x20e2ccc17662fc1d, 0xea7a7dcb2e787c01, 0xf4e85a418b9c4f80)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng  = Random.Xoshiro(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::CPUDevice) (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev =     reactant_device()\n",
    "gpu_dev = gpu_device(2)\n",
    "cpu_dev = cpu_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lux Models are decoupled from their parameters and states. Hence, we have to obtain them using the appropriate Initializers. For this, Lux comes with `Lux.setup`, which iterates through the model and returns the parameter and state arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(Dense(28*28, 128, relu), Dense(128, 10), softmax)\n",
    "ps, st = Lux.setup(rng, model)\n",
    "ps_dev = ps |> dev\n",
    "st_dev = st |> dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×60 CuArray{Float32, 2, CUDA.DeviceMemory}:\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  1.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  1.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_dev = Random.randn(Float32, 28*28, 60) |> dev;\n",
    "y_dev= Random.rand(0:9, 60) |>(x-> onehotbatch(x, 0:9)) .|> Float32 |> dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "cannot copy Ptr{CUDA.CUctx_st} @0x0000000036717300 of type Ptr{CUDA.CUctx_st}",
     "output_type": "error",
     "traceback": [
      "cannot copy Ptr{CUDA.CUctx_st} @0x0000000036717300 of type Ptr{CUDA.CUctx_st}\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base ./error.jl:35\n",
      "  [2] create_result(tocopy::Ptr{CUDA.CUctx_st}, path::NTuple{7, Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:47\n",
      "  [3] create_result(tocopy::CuContext, path::NTuple{6, Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:56\n",
      "  [4] create_result(tocopy::CUDA.DeviceMemory, path::NTuple{5, Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:56\n",
      "  [5] create_result(tocopy::CUDA.Managed{CUDA.DeviceMemory}, path::NTuple{4, Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:56\n",
      "  [6] create_result(tocopy::GPUArrays.RefCounted{CUDA.Managed{CUDA.DeviceMemory}}, path::Tuple{Int64, Int64, Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:56\n",
      "  [7] create_result(tocopy::GPUArrays.DataRef{CUDA.Managed{CUDA.DeviceMemory}}, path::Tuple{Int64, Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:56\n",
      "  [8] create_result(tocopy::CuArray{Float32, 2, CUDA.DeviceMemory}, path::Tuple{Int64}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:56\n",
      "  [9] create_result(tocopy::Tuple{CuArray{Float32, 2, CUDA.DeviceMemory}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}, path::Tuple{}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:151\n",
      " [10] codegen_unflatten!(linear_args::Vector{Union{ReactantCore.MissingTracedValue, Reactant.TracedRArray, Reactant.TracedRNumber}}, preserved_args::Vector{Tuple{Union{ReactantCore.MissingTracedValue, Reactant.TracedRArray, Reactant.TracedRNumber}, Int64}}, concretized_res_names::Vector{Symbol}, linear_results::Vector{Union{ReactantCore.MissingTracedValue, Reactant.TracedRArray, Reactant.TracedRNumber}}, concrete_result::Tuple{CuArray{Float32, 2, CUDA.DeviceMemory}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}, result_stores::Dict{Tuple, Symbol}, path_to_shard_info::Nothing, linear_result_shard_info::Tuple{}, sharding_mesh::Nothing)\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:1170\n",
      " [11] compile(f::Chain{@NamedTuple{layer_1::Dense{typeof(relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::WrappedFunction{typeof(softmax)}}, Nothing}, args::Tuple{CuArray{Float32, 2, CUDA.DeviceMemory}, @NamedTuple{layer_1::@NamedTuple{weight::CuArray{Float32, 2, CUDA.DeviceMemory}, bias::CuArray{Float32, 1, CUDA.DeviceMemory}}, layer_2::@NamedTuple{weight::CuArray{Float32, 2, CUDA.DeviceMemory}, bias::CuArray{Float32, 1, CUDA.DeviceMemory}}, layer_3::@NamedTuple{}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}; sync::Bool, kwargs::@Kwargs{client::Nothing, no_nan::Bool, optimize::Bool})\n",
      "    @ Reactant.Compiler ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:1467\n",
      " [12] top-level scope\n",
      "    @ ~/.julia/packages/Reactant/qEA3Y/src/Compiler.jl:962"
     ]
    }
   ],
   "source": [
    "compiled_model = @compile model(x_dev, ps_dev, st_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the MNIST data\n",
    "The MNIST Dataset is a common benchmark dataset, hence it is included in MLDatasets.jl. Here, we can download it quite easily.\n",
    "(You propably have to do that in the REPL, since it requires you to accept the download.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets, MLUtils, OneHotArrays, Plots, Images, Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :test\n",
       "  features  =>    28×28×10000 Array{Float32, 3}\n",
       "  targets   =>    10000-element Vector{Int64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNIST_train  = MLDatasets.MNIST(:train)\n",
    "MNIST_test   = MLDatasets.MNIST(:test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST_train.features |> x-> reshape(x, (28*28, size(x,3)))\n",
    "test_data = MNIST_test.features   |> x-> reshape(x, (28*28, size(x,3)))\n",
    "train_labels = MNIST_train.targets |> x-> onehotbatch(x,0:9) .|> Float32\n",
    "test_labels = MNIST_test.targets |> x-> onehotbatch(x,0:9) .|> Float32;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceIterator{CUDADevice{CuDevice}, DataLoader{BatchView{@NamedTuple{data::Matrix{Float32}, label::Matrix{Float32}}, ObsView{@NamedTuple{data::Matrix{Float32}, label::Matrix{Float32}}, Vector{Int64}}, Val{nothing}}, Bool, :serial, Val{nothing}, @NamedTuple{data::Matrix{Float32}, label::Matrix{Float32}}, TaskLocalRNG}}(CUDADevice{CuDevice}(CuDevice(0)), DataLoader(::@NamedTuple{data::Matrix{Float32}, label::Matrix{Float32}}, shuffle=true, batchsize=60))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader = DataLoader((data = train_data, label = train_labels), batchsize=60, shuffle=true) |> dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_function (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss_function(model, ps, st, x, y)\n",
    "    pred, _ = model(x, ps, st)\n",
    "    return CrossEntropyLoss()(pred, y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_gradient (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss_gradient(model, ps, st, x, y)\n",
    "    return Enzyme.gradient(Enzyme.Reverse, Const(loss_function),\n",
    "                           Const(model), ps, Const(st), Const(x), Const(y))[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_loss_grad = @compile loss_gradient(model, ps_dev, st_dev, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Loop\n",
    "In the following, let us define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_model(model, ps, st, dataloader, optimizer, iterations::Integer)\n",
    "    train_state = Training.TrainState(model, ps, st, optimizer)\n",
    "    for iter in 1:iterations\n",
    "        for (x,y) in dataloader\n",
    "            _, loss, _, train_state = Training.single_train_step!(AutoZygote(), CrossEntropyLoss(), (x,y), train_state)\n",
    "        end\n",
    "    end\n",
    "    return train_state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState\n",
       "    model: Chain{@NamedTuple{layer_1::Dense{typeof(relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::WrappedFunction{typeof(softmax)}}, Nothing}((layer_1 = Dense(784 => 128, relu), layer_2 = Dense(128 => 10), layer_3 = WrappedFunction(softmax)), nothing)\n",
       "    # of parameters: 101770\n",
       "    # of states: 0\n",
       "    optimizer: Descent(0.01)\n",
       "    step: 1000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "end_state = train_model(model, ps_dev, st_dev, train_dataloader, Optimisers.Descent(0.01), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps_trained = end_state.parameters\n",
    "st_trained = end_state.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function accuracy(model, ps, st, x, y)\n",
    "    pred, _ = model(x,ps,st)\n",
    "    return mean(onecold(pred |> cpu_dev) .== onecold(y |> cpu_dev))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: reducing over an empty collection is not allowed; consider supplying `init` to the reducer",
     "output_type": "error",
     "traceback": [
      "ArgumentError: reducing over an empty collection is not allowed; consider supplying `init` to the reducer\n",
      "\n",
      "Stacktrace:\n",
      " [1] _empty_reduce_error()\n",
      "   @ Base ./reduce.jl:319\n",
      " [2] reduce_empty(::Base.MappingRF{typeof(identity), typeof(+)}, ::Type{Union{}})\n",
      "   @ Base ./reduce.jl:322\n",
      " [3] reduce_empty_iter\n",
      "   @ ./reduce.jl:381 [inlined]\n",
      " [4] mapreduce_empty_iter(f::Function, op::Function, itr::Base.SkipMissing{Vector{Missing}}, ItrEltype::Base.HasEltype)\n",
      "   @ Base ./reduce.jl:377\n",
      " [5] mean(f::typeof(identity), itr::Base.SkipMissing{Vector{Missing}})\n",
      "   @ Statistics ~/.julia/packages/Statistics/gbcbG/src/Statistics.jl:64\n",
      " [6] mean(itr::Base.SkipMissing{Vector{Missing}})\n",
      "   @ Statistics ~/.julia/packages/Statistics/gbcbG/src/Statistics.jl:44\n",
      " [7] |>(x::Base.SkipMissing{Vector{Missing}}, f::typeof(mean))\n",
      "   @ Base ./operators.jl:926\n",
      " [8] top-level scope\n",
      "   @ ~/Documents/project/jax_intro/lux_intro/lux_intro_env/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X32sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "test_dataloader  = DataLoader((data = test_data, label = test_labels), batchsize=60) |> dev\n",
    "map(test_dataloader) do (x,y)\n",
    "missing#   size(x) == (784,60) ? accuracy(compiled_model, ps_trained, st_trained, x,y) : missing\n",
    "end |> skipmissing |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_image (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function show_image(flat_image)\n",
    "    #input is (28*28)\n",
    "    flat_image = reshape(flat_image, (28,28))\n",
    "    Gray.(flat_image')\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
