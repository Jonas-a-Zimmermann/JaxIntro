{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon now we have Used JAX directly. Using JAX for ML seems like a straightforward idea, just as we implemented a MLP model for MNIST in the HelloWorld_Cuda notebook. Ofcourse other people (with a lot more ressources than we have) did this aswell. One such stack ontop of JAX is Google's FLAX, which we will introduce in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "DEFAULTGPU = jax.devices()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLAX comes with different APIs. For some reason or another, the current recommendation is to use 'nnx'.\n",
    "Note that in nnx models are stateful! This is a big difference compared to pure the pure JAX model we implemented, or Julia's LUX library (where the developement went the other way, ironically dropping the 'F' from FLUX')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'nnx' comes with a plethora of pre defined layers. As an example, consider again the MNIST problem, where we defined a MLP using a \"Dense Layer\" and a decoupled parameter Constructor (again, see the Hello World Notebook):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def denseLayer(params, image):\n",
    "    W,b = params\n",
    "    return jnp.dot(W, image) + b\n",
    "\n",
    "def denseLayerConstructor(key, indims, outdims, weight=fnc.glorot_normal, bias=fnc.glorot_uniform):\n",
    "    W_key, b_key = random.split(key)\n",
    "    W = weight(W_key, (outdims, indims))\n",
    "    b = bias(b_key, (outdims,))\n",
    "    return (W,b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, in 'nnx' Layers shall be statefull, that is for the Dense layer instead of using the combination 'function + parameters' we will combine them into a single object. Hence the Dense Layer would look something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class NNXDenseLayer(nnx.Module):\n",
    "    def __init__(self, key, indims, outdims, weight=fnc.glorot_normal, bias=fnc.glorot_uniform):\n",
    "        W_key, B_key = random.split(key)\n",
    "        self.w = nnx.Param(weight(W_key, (outdims, indims)))\n",
    "        self.b = nnx.Param(bias(B_key, (outdims, )))\n",
    "        self.din, self.dout = indims, outdims\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.w + self.b\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now reimplement our MLP model, using nnx's build-in dense layer:\n",
    "(HA! we cant, beause nnx's initializers do not work with bias terms. We could use our own initializer, that does successfully dispatch on the bias, or we use the dafault zero initializer. Maybe someday they will fix it. (Or you can do so yourself.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.nnx import initializers  as nnx_init\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "    def __init__(self, rng):\n",
    "        self.lin1 = nnx.Linear(28*28, 128, rngs= rng, kernel_init = nnx_init.glorot_normal())\n",
    "        self.lin2 = nnx.Linear(128, 10, rngs = rng, kernel_init=nnx_init.glorot_normal())\n",
    "\n",
    "    @nnx.jit\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.lin1(x))\n",
    "        x = nnx.softmax(self.lin2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(nnx.Rngs(jax.random.PRNGKey(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to put in on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nnx.state(model)\n",
    "state = jax.device_put(state, DEFAULTGPU)\n",
    "nnx.update(model, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "We will reuse the same Dataloading shenannigans as in the Hello World notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/Documents/project/jax_intro/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk;\n",
    "import pathlib\n",
    "if not (pathlib.Path(pathlib.Path.cwd() / \"MNIST.hf\")).exists():\n",
    "    MNIST = load_dataset(\"ylecun/mnist\")\n",
    "    MNIST.save_to_disk(\"MNIST.hf\")\n",
    "else:\n",
    "    MNIST = load_from_disk(\"MNIST.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = MNIST[\"train\"].with_format(\"jax\")\n",
    "test_data = MNIST[\"test\"].with_format(\"jax\")\n",
    "training_inputs = training_data[\"image\"].astype(jnp.float32) / 255\n",
    "training_labels = jax.nn.one_hot(training_data[\"label\"], 10)\n",
    "test_inputs = test_data[\"image\"].astype(jnp.float32) / 255  \n",
    "test_labels = jax.nn.one_hot(test_data[\"label\"], 10)\n",
    "\n",
    "training_inputs = jnp.reshape(training_inputs, (60000, 28*28))\n",
    "test_inputs = jnp.reshape(test_inputs, (10000, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(object):\n",
    "    def __init__(self, inputs, labels, batch_size):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = inputs.shape[0] // batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(self.num_batches):\n",
    "            start = i * self.batch_size\n",
    "            end = (i + 1) * self.batch_size\n",
    "            yield (self.inputs[start:end], self.labels[start:end])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def shuffle(self, rng_key):\n",
    "        perm = jax.random.permutation(rng_key, self.inputs.shape[0])\n",
    "        self.inputs = self.inputs[perm]\n",
    "        self.labels = self.labels[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = dataloader(training_inputs, training_labels, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike in JAX we do not have to explicitly interate through our batch, i.e. call vmap. nnx does that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_in.shape (60, 784), type <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "for batch in training_dataloader:\n",
    "    batch_in, batch_label = batch\n",
    "    print(f\"batch_in.shape {batch_in.shape}, type {type(batch_in)}\")\n",
    "    model(batch_in)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function\n",
    "\n",
    "The loss function takes the model and compares it to the correct labels. Note that we have a one-hot encoding for the label classes. We again want to implement the Cross Entropy Loss $$ L = -\\sum_x p(x)log(q(x)) ,$$ however <s>nnx</s> optax already contains it.\n",
    "Optax is Google's optimization package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def lossfn(model, inputs, labels):\n",
    "    prediction = model(inputs)\n",
    "    return optax.softmax_cross_entropy(logits=prediction, labels=labels).mean(), prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimizer\n",
    "In the HelloWorld notebook we implemented our own version of gradient descent and a custom `update` function. This is ofcourse not needed, when we use optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nnx.Optimizer(model, optax.sgd(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Let us begin with the training loop. For this iterate through batches shuffle them and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note how we dont @partial(jit, ....)\n",
    "@nnx.jit\n",
    "def __train_step(model, batch, optimizer):\n",
    "    batch_in, batch_label = batch\n",
    "    #seems inefficient to make a function definition here, but its compiled away.\n",
    "    gradfn = nnx.value_and_grad(lossfn, has_aux=True) #has_aux means that we return a tuple (x, aux) where x is differentiated.\n",
    "    (loss,logits), grads = gradfn(model, batch_in, batch_label)\n",
    "    #the model is semi-implicit...\n",
    "    optimizer.update(grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nnx comes with their own Metric function. However, to keep the notebooks somewhat comparable, we use our old accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def accuracy(model, inputs, labels):\n",
    "    predictions = model(inputs)\n",
    "    return (predictions.argmax(axis=1) == labels.argmax(axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = jax.random.choice(jax.random.PRNGKey(0), len(test_inputs)-1, (1000,), replace=False)\n",
    "train_acc_inputs = training_inputs[keys].to_device(DEFAULTGPU)\n",
    "train_acc_labels = training_labels[keys].to_device(DEFAULTGPU)\n",
    "test_labels = test_labels.to_device(DEFAULTGPU)\n",
    "test_inputs = test_inputs.to_device(DEFAULTGPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMEPOCHS = 1000\n",
    "\n",
    "\n",
    "train_accs = []\n",
    "test_accs  = []\n",
    "\n",
    "model = MLP(nnx.Rngs(jax.random.PRNGKey(4)))\n",
    "state = nnx.state(model)\n",
    "state = jax.device_put(state, DEFAULTGPU)\n",
    "nnx.update(model, state)    \n",
    "\n",
    "training_dataloader = dataloader(training_inputs.to_device(DEFAULTGPU), training_labels.to_device(DEFAULTGPU), 128)\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(learning_rate=0.001))\n",
    "key = jax.random.PRNGKey(1234)\n",
    "for epoch in range(NUMEPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    training_dataloader.shuffle(subkey)\n",
    "    for batch in training_dataloader:\n",
    "        input, labels = batch\n",
    "        #input  = input.to_device(DEFAULTGPU)\n",
    "        #labels = labels.to_device(DEFAULTGPU)\n",
    "        __train_step(model, (input, labels), optimizer)\n",
    "\n",
    "    train_acc = accuracy(model, train_acc_inputs, train_acc_labels)\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = accuracy(model, test_inputs, test_labels)\n",
    "    test_accs.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import FunctionCollection as fnc\n",
    "plt.rcParams.update({\n",
    "    'font.size': 32,\n",
    "    'axes.labelsize': 18,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'legend.fontsize': 18,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "\n",
    "plt.figure(dpi=300, figsize=(16,9))\n",
    "plt.plot(test_accs, label=\"Test Accuracy\")\n",
    "plt.plot(train_accs, label=\"Train Accuracy\")\n",
    "xlabel = plt.xlabel(\"Epoch\")\n",
    "ylabel = plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.savefig(\"plots/accuracy.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plot_displayer = fnc.PlotDisplay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"text-align: center\">\n",
       "            <img src=\"plots/accuracy.png?v=1738859000657\" width=\"800\"/>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plot_displayer.update() #This forces the Renderer to reload the plot\n",
    "plot_displayer.show(\"plots/accuracy.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
