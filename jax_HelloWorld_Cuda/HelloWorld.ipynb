{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook Contains a Hello World example using JAX on CUDA backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "devices = jax.devices()\n",
    "print(devices)\n",
    "runtime_device = devices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Multiple CudaDevices are shown here, the installation was successfull. (On my workstation that has 4 GPU's that is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jax Numpy\n",
    "\n",
    "Jax main features are exposed in a numpy like API, accessible via Jax Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import FunctionCollection as fnc\n",
    "from jax import random\n",
    "rng = random.PRNGKey(0)\n",
    "JaxArray = random.normal(rng,(10000,10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the JaxArray is indeed a JaxArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(JaxArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the Array onto the GPU, then function calls will be excuted as Kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JaxArrayCPU = JaxArray.copy().to_device(jax.devices('cpu')[0])\n",
    "JaxArrayGPU = JaxArray.copy().to_device(runtime_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will demonstrate a runtime example running the product $x*x^T$ on cpu and gpu, respectively.\n",
    "The corresponding function is defined in the FunctionCollection (fnc) as runtime_example.\n",
    "Before timing, we run the function once, in order to get the compilation done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cpu Time:  1.5748882293701172\n",
      "Gpu Time:  0.02760004997253418\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "fnc.runtime_example(JaxArrayCPU)\n",
    "fnc.runtime_example(JaxArrayGPU)\n",
    "#begin Timing on the CPU\n",
    "start = time.time()\n",
    "result_cpu = fnc.runtime_example(JaxArrayCPU)\n",
    "result_cpu.block_until_ready()\n",
    "end = time.time()\n",
    "print(\"Cpu Time: \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "result_gpu = fnc.runtime_example(JaxArrayGPU)\n",
    "result_gpu.block_until_ready()\n",
    "end = time.time()\n",
    "print(\"Gpu Time: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Runtime acceleration suggests an running on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "Now that we have seen that Jax is indeed running, lets do the ML Hello World of MNIST Classification!\n",
    "Note that JAX is a AutoDiff Library, NOT a NN library; hence it does not come with a lot of QOL features.\n",
    "\n",
    "<a href=\"#neuralnetwork\"> Neural Network </a> <br>\n",
    "<a href=\"#dataloader\"> Data Loading </a> <br>\n",
    "<a href=\"#training\"> Training Loop </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the Neural Network\n",
    "<a id=\"neuralnetwork\"> </a>\n",
    "The most important difference between implementation in Torch, TensorFlow and Flux is that JAX does not come with the Tensor approach, where an entire batch is making up a Tensor of size (batch, datadims) (or the other way around). Instead we have to define our network on singular datapoints and then vectorize (\"batching it up\") later.\n",
    "Further notice that the parameters and the model architecture are decoupled, making inspecting the params as well as replacing them straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.nn as jnn\n",
    "@jax.jit\n",
    "def denseLayer(params, image):\n",
    "    W,b = params\n",
    "    return jnp.dot(W, image) + b\n",
    "\n",
    "def denseLayerConstructor(key, indims, outdims, weight=fnc.glorot_normal, bias=fnc.glorot_uniform):\n",
    "    W_key, b_key = random.split(key)\n",
    "    W = weight(W_key, (outdims, indims))\n",
    "    b = bias(b_key, (outdims,))\n",
    "    return (W,b)\n",
    "\n",
    "@jax.jit\n",
    "def model(params, image):\n",
    "    activations = image\n",
    "    for param in params[:-1]:\n",
    "        activations = denseLayer(param, activations)\n",
    "        activations = jnn.relu(activations)\n",
    "    activations = denseLayer(params[-1], activations)\n",
    "    return jnn.softmax(activations)\n",
    "\n",
    "def modelConstructor(key, layerdims):\n",
    "    return [denseLayerConstructor(key, layerdims[i], layerdims[i+1]) for i in range(len(layerdims)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParameters = modelConstructor(rng, (28*28,128,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that it indeed works, lets generate a random Image in vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.16604736, 0.2734799 , 0.09123447, 0.08354109, 0.12440692,\n",
       "       0.06872484, 0.0261414 , 0.1224124 , 0.02137834, 0.02263325],      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = jax.random.uniform(rng, 28*28, minval=0, maxval=1)\n",
    "model(modelParameters, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Order to apply this function to a batch of Data, we can use JAX's vmap. \n",
    "vmap uses the keywoard in_axes to specify which inputs we want to batch over;\n",
    "The signature of the function remains the same, such that you can imagine\n",
    "```\n",
    "predictor(params, batch) = [model(params, image) for image in batch]\n",
    "```\n",
    "Since we do not want to iterate over params, but instead feed them all, we put in_axes=(None,...).\n",
    "The second argument specifies the axis we want to iterate the batch over. Since our batches will be designed as (batchsize, imagesize), we want to iterate over the 0th axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = jax.vmap(model, in_axes=(None,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this now Indeed works on a batch of Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_images = jax.random.uniform(rng, (10,28*28), minval=0, maxval=1)\n",
    "predictions = predictor(modelParameters, batched_images)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the Loss Function\n",
    "We will implement the Categorial Cross Entropy \n",
    "    $$ L = -\\sum_x p(x)log(q(x)) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCEloss(params, batch):\n",
    "    inputs, targets = batch\n",
    "    predictions = predictor(params, inputs)\n",
    "    return -jnp.mean(jnp.sum(targets * jnp.log10(predictions), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading <a id=\"dataloader\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/Documents/project/jax_intro/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "if not (pathlib.Path(pathlib.Path.cwd() / \"MNIST.hf\")).exists():\n",
    "    MNIST = load_dataset(\"ylecun/mnist\")\n",
    "    MNIST.save_to_disk(\"MNIST.hf\")\n",
    "else:\n",
    "    MNIST = load_from_disk(\"MNIST.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have to implement our own custom Dataloader. Luckily this is just a generator, with the added capabilities of scrambling our data. We will do that on the CPU and afterwards shift over to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = MNIST[\"train\"].with_format(\"jax\")\n",
    "test_data = MNIST[\"test\"].with_format(\"jax\")\n",
    "training_inputs = training_data[\"image\"].astype(jnp.float32) / 255\n",
    "training_labels = jax.nn.one_hot(training_data[\"label\"], 10)\n",
    "test_inputs = test_data[\"image\"].astype(jnp.float32) / 255  \n",
    "test_labels = jax.nn.one_hot(test_data[\"label\"], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple Example, we will use a Dense NN on the MNIST data, hence we need to flatten the input first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = training_inputs.reshape(training_inputs.shape[0], -1)\n",
    "test_inputs = test_inputs.reshape(test_inputs.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, lets write a dataloader in order to feed batches into our Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(object):\n",
    "    def __init__(self, inputs, labels, batch_size):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = inputs.shape[0] // batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(self.num_batches):\n",
    "            start = i * self.batch_size\n",
    "            end = (i + 1) * self.batch_size\n",
    "            yield (self.inputs[start:end], self.labels[start:end])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def shuffle(self):\n",
    "        perm = jax.random.permutation(rng, self.inputs.shape[0])\n",
    "        self.inputs = self.inputs[perm]\n",
    "        self.labels = self.labels[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very basic implementation of an Dataloader is a iterable, capable of shuffling data. Here, the Data must lie within the RAM, however this is a more or less artificial constraint, which we will break as soon as we have larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop<a id=training> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with a dataloader and a neural network we can now start to think about training the model.\n",
    "For this we need to be able to backpropagate through our neural network and to collect the gradients.\n",
    "This is the job of JAX, and the whole reason we use this software: It can automatically step through a function call tree and (effciently) compute gradients with respect to the input parameters. This is nice, because it means we dont have to do that per hand. In theory, (afaik) manually derived derivatives are more efficient, however the cost in additional developement time is almost never worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = jax.grad(CCEloss)(modelParameters, (training_inputs[:10], training_labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this gradient in the update function, which will be accelerated by jax compilation aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, batch, lr=0.001):\n",
    "    grads = jax.grad(CCEloss)(params, batch)\n",
    "    return [(w - lr *dw, b -lr*db) for (w,b), (dw,db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the training loop is straightforward an iteration throug our epochs and batches, applying the update function on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTINGS FOR THE TRAINING\n",
    "lr = 0.001\n",
    "NUMEPOCHS = 100\n",
    "BATCHSIZE = 60\n",
    "\n",
    "train_loader = dataloader(training_inputs, training_labels, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParameters = [(w.to_device(runtime_device), b.to_device(runtime_device)) for (w,b) in modelParameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    train_loader.shuffle()\n",
    "    for batch in train_loader:\n",
    "        input, labels = batch\n",
    "        input  = input.to_device(runtime_device)\n",
    "        labels = labels.to_device(runtime_device)\n",
    "        modelParameters = update(modelParameters, (input, labels), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propably also want to measure how good our model is performing. For this, let us define the accuracy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def accuracy(parameters, batch):\n",
    "    inputs, targets = batch\n",
    "    predictions = predictor(parameters, inputs)\n",
    "    return jnp.mean(jnp.argmax(predictions, axis=1) == jnp.argmax(targets, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding this to the training Loop above, keeping track of the accuracy, and actually evaluating the Loop for mroe than 1 EPOCH leads to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = jax.random.choice(rng, len(test_inputs)-1, (1000,), replace=False)\n",
    "train_acc_inputs = training_inputs[keys].to_device(runtime_device)\n",
    "train_acc_labels = training_labels[keys].to_device(runtime_device)\n",
    "test_labels = test_labels.to_device(runtime_device)\n",
    "test_inputs = test_inputs.to_device(runtime_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_accs = []\n",
    "test_accs  = []\n",
    "for epoch in range(NUMEPOCHS):\n",
    "    train_loader.shuffle()\n",
    "    for batch in train_loader:\n",
    "        input, labels = batch\n",
    "        input  = input.to_device(runtime_device)\n",
    "        labels = labels.to_device(runtime_device)\n",
    "        modelParameters = update(modelParameters, (input, labels), lr=lr)\n",
    "\n",
    "    train_acc = accuracy(modelParameters, (training_inputs.to_device(runtime_device), training_labels.to_device(runtime_device)))\n",
    "    train_accs.append(train_acc)\n",
    "    test_acc = accuracy(modelParameters, (test_inputs, test_labels))\n",
    "    test_accs.append(test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 16,\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Computer Modern Roman']\n",
    "})\n",
    "\n",
    "plt.figure(dpi=300, figsize=(16,9))\n",
    "plt.plot(test_accs, label=\"Test Accuracy\")\n",
    "plt.plot(train_accs, label=\"Train Accuracy\")\n",
    "xlabel = plt.xlabel(\"Epoch\")\n",
    "ylabel = plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.savefig(\"plots/accuracy.png\")\n",
    "plt.close()\n",
    "plot_displayer = fnc.PlotDisplay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"text-align: center\">\n",
       "            <img src=\"plots/accuracy.png?v=1738343410159\" width=\"800\"/>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_displayer.update() #This forces the Renderer to reload the plot\n",
    "plot_displayer.show(\"plots/accuracy.png\", width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we see that we successfully implemented a MNIST classifier, just like a few billion people before us! But fear not, this is still an achievement, since it is the \"Hello World!\" of Machine Learning, and hence that we successfully installed all required packages, and understood the very basics of our language (in this case JAX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
